{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "### 简介\n",
    "Tokenization 的主要目的是将文本分解成更小的单位(Tokens)，减小模型输入数据的内在结构复杂度(从句子变为单词序列)，从而简化模型训练的难度。同时将字符的序列转化为Token序号的序列，便于模型输入。\n",
    "\n",
    "Tokenization 首先确定语言的词表划分粒度，一般可分为：\n",
    "* 字符级：将文本分解为字符。\n",
    "* 单词级：将文本分解为单词。\n",
    "* 子词级：将单词进一步分解为更小的有意义单元（如前缀、后缀）。\n",
    "\n",
    "之后使用预定义的规则来识别 tokens, 或使用统计或机器学习技术来识别最优的 token 切分方式。例如，BPE（Byte Pair Encoding）或 SentencePiece。\n",
    "\n",
    "最后实现一组文本序列和Tokens序列之间相互转化的函数，即可完成Tokenization部分。\n",
    "\n",
    "### 实验要求\n",
    "\n",
    "1. 实现字符级切分的简单tokenizer， 由 字符表， 字符到token的 encoder()函数 和 token到字符的 decoder() 函数组成。\n",
    "2. 调用 现有的tokenizer实现，比如openai 的tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataPath:str\n",
    "        ):\n",
    "        with open(dataPath,\"r\",encoding=\"utf-8\") as f:\n",
    "            self.dataset = f.read()\n",
    "        self.generate_vocabulary()\n",
    "\n",
    "    def generate_vocabulary(\n",
    "        self,\n",
    "        ):\n",
    "        self.char2index = {}\n",
    "        self.index2char = {}\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        \"\"\"\n",
    "        unique_chars = list(set(self.dataset))\n",
    "        unique_chars.sort()\n",
    "        for i, char in enumerate(unique_chars):\n",
    "            self.char2index[char] = i+1\n",
    "            self.index2char[i+1] = char\n",
    "        self.index2char[0] = \" \"\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        sentence : str,\n",
    "        ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        例子, 假设A-Z 对应的token是1-26, 句子开始，结束符号的token是0。\n",
    "        input  : \"ABCD\"\n",
    "        output : Tensor([0,1,2,3]) \n",
    "\n",
    "        注意: 为了后续实验方便，输出Tensor的数据类型dtype 为torch.long。\n",
    "        \"\"\"\n",
    "        sentence = [self.char2index[char] for char in sentence]\n",
    "        sentence = [0] + sentence\n",
    "        return torch.tensor(sentence, dtype=torch.long, device=device)\n",
    "\n",
    "    def decode(\n",
    "        self,\n",
    "        tokens : torch.Tensor,\n",
    "        ) -> str:\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        例子, 假设A-Z 对应的token是1-26, 句子开始，结束符号的token是0。\n",
    "        input : Tensor([0,1,2,3]) \n",
    "        output : \"ABCD\"\n",
    "        \"\"\"\n",
    "        sentence = [self.index2char[index] for index in tokens]\n",
    "        sentence = sentence[1:]\n",
    "        return \"\".join(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义 dataloader 和 dataset\n",
    "\n",
    "为了高效加载数据，我们需要把输入文件接入 PyTorch 的数据加载器中。在这里我们定义 `ShakespeareDataset` 类用于加载数据集，用 PyTorch 的 `DataLoader` 类来实现数据加载。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, filepath, tokenizer, chunk_size):\n",
    "        self.tokenizer = tokenizer\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        # text = \"\\n\".join([line for line in text.splitlines() if line.strip()])\n",
    "        # text = re.sub(r'^.*:$', '', text, flags=re.MULTILINE)\n",
    "        # text = \"\\n\".join([line for line in text.splitlines() if line.strip()])\n",
    "        self.encoded = self.tokenizer.encode(text)\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded) - self.chunk_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #TODO: 提取一段文本(长度为 chunk_size）作为输入，以及这段文本的每一个字符的下一个字符作为标签\n",
    "        # example(not correspond to real text): chunk = tensor([ 0, 20, 49, 58, 59])\n",
    "        #         label = tensor([20, 49, 58, 59, 19])\n",
    "        # decoded chunk: \"The \"\n",
    "        # decoded label: \"he T\"\n",
    "        chunk = self.encoded[idx:idx+self.chunk_size]\n",
    "        label = self.encoded[idx+1:idx+self.chunk_size+1]\n",
    "\n",
    "        return chunk, label\n",
    "\n",
    "tokenizer = Tokenizer(dataPath=\"input.txt\")\n",
    "\n",
    "def create_dataloader(filepath, tokenizer, chunk_size, batch_size, shuffle=True):\n",
    "    dataset = ShakespeareDataset(filepath, tokenizer, chunk_size)\n",
    "    train_dataset,val_dataset = torch.utils.data.random_split(dataset,[int(len(dataset)*0.8),len(dataset)-int(len(dataset)*0.8)])\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "\n",
    "train_dataloader,val_dataloader = create_dataloader('input.txt', tokenizer, chunk_size=200, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意力的计算公式为：\n",
    "$$\n",
    "Head = Attention(x)=Softmax(M\\cdot QK^T)V\\\\\n",
    "Q=xW_{q},K=xW_{k}, V=xW_{v}\n",
    "$$\n",
    "这里实现的一些数学技巧可以参见attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadAttention(nn.Module):\n",
    "    def __init__(self, seq_len:int, embed_size:int, hidden_size:int):\n",
    "        super().__init__()\n",
    "        # embed_size: dimension for input embedding vector\n",
    "        # hidden_size: dimension for hidden vector. eg. x:(..., embed_size) --to_q--> query_vector:(..., hidden_size)\n",
    "\n",
    "        # a triangular bool matrix for mask\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(seq_len, seq_len)))\n",
    "        \n",
    "        # TODO: init three matrix, to_q, to_k, to_v.\n",
    "        self.to_q = nn.Linear(embed_size, hidden_size)\n",
    "        self.to_k = nn.Linear(embed_size, hidden_size)\n",
    "        self.to_v = nn.Linear(embed_size, hidden_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # input: (batch_size, seq_len, embed_size)\n",
    "        # return (batch_size, seq_len, hidden_size)\n",
    "        # TODO: implement the attention mechanism\n",
    "        q = self.to_q(inputs)\n",
    "        k = self.to_k(inputs)\n",
    "        v = self.to_v(inputs)\n",
    "\n",
    "        # q, k, v: (batch_size, seq_len, hidden_size)\n",
    "        # attn: (batch_size, seq_len, seq_len)\n",
    "        attn = torch.bmm(q, k.transpose(1, 2)) / (k.size(-1) ** 0.5)\n",
    "        attn = attn.masked_fill(self.tril == 0, float('-inf'))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        return torch.bmm(attn, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer中使用的注意力机制时会使用多个注意力头，期望每个注意力头能够注意到不同的信息。\n",
    "所以实际公式需要修改如下\n",
    "$$\n",
    "MultiHeadAttention(x)=[Head_0, Head_1,...,Head_h]W_o\\\\\n",
    "Head_i = Attention(x)=Softmax(M\\cdot Q_iK_i^T)V_i\\\\\n",
    "Q_i=xW_{iq},K=xW_{ik}, V=xW_{iv}\n",
    "$$\n",
    "在搭建网络的过程中，同学们可能会用到nn.ModuleList这个库，每个$Head_i$的计算可以直接使用上面已经实现的单头注意力计算。\n",
    "最后对于这些注意力头再使用一个简单的线性层/矩阵$W_o$汇总信息即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    # MultiHeadAttention is consist of many HeadAttention output.\n",
    "    # concat all this head attention output o_i, then merge them with a projection matrix W_o, as [o_1, o_2, ...] x W_o\n",
    "    # The reason for using multi-head attention is that we want each head to be able to extract different features\n",
    "    def __init__(self, n_heads:int, head_size:int, seq_len:int, embed_size:int):\n",
    "        # n_heads is the number of head attention\n",
    "        # head_size is the hidden_size in each HeadAttention\n",
    "        super().__init__()\n",
    "        head_size = embed_size // n_heads\n",
    "        #TODO: implement heads and projection\n",
    "        self.heads = nn.ModuleList([HeadAttention(seq_len, embed_size, head_size) for _ in range(n_heads)])\n",
    "        self.projection = nn.Linear(n_heads * head_size, embed_size)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # input: (batch_size, seq_len, embed_size), make sure embed_size=n_heads x head_size\n",
    "        # return: (batch_size, seq_len, embed_size)\n",
    "        # TODO:\n",
    "        head_outputs = [head(inputs) for head in self.heads]\n",
    "        return self.projection(torch.cat(head_outputs, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 专家网络 Expert\n",
    "\n",
    "Expert即为标准Transformer中的FeedForward模块。\n",
    "\n",
    "在经过MultiHeadAttention 模块后，seq_len中的每一个Embedding都对应了前文信息的加权求和。在经过FeedForward模块时，模型对每一个位置的Embedding进行了两次线性变换和一次非线性变换，可以视为对当前语境下的信息进行加工。知识编辑的一些研究表明，FeedForword 模块参数包含了大量的事实性知识。\n",
    "\n",
    "一个直观的想法是，类比于MultiHeadAttention，我们在每一层训练多个FeedForward模块，对于不同位置的Embedding使用不同的FeedForward模块处理对应的信息。就好像每层有多个Expert,每个Expert都负责处理一类数据的深加工，因此我们称FeedForward为Expert。\n",
    "\n",
    "实现方面:\n",
    "\n",
    "FeedForward层由两层简单的线性层组成，对于一个(batch_size, seq_len, embed_size)输入的向量x\n",
    "只在最后一个维度上进行计算，以实现词的特征维度上的交互(注意力机制是词之间的交互)。\n",
    "其首先用一个线性层将x最后一维扩大至原先4倍，然后继续用一个线性层还原回原先的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self, embed_size:int):\n",
    "        super().__init__()\n",
    "        #TODO: init two linear layer\n",
    "        self.fc1 = nn.Linear(embed_size, 4*embed_size)\n",
    "        self.fc2 = nn.Linear(4*embed_size, embed_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs: (batch_size, seq_len, embed_size)\n",
    "        # -> mid: (batch_size, seq_len, 4 x embed_size)\n",
    "        # -> outputs: (batch_size, seq_len, embed_size)\n",
    "        mid = F.relu(self.fc1(inputs))\n",
    "        return self.fc2(mid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 选通网络 TopkRouter\n",
    "\n",
    "在实现了单个Expert后，我们要设计一个选通网络决策每个Embedding要使用那个Expert计算\n",
    "\n",
    "\n",
    "### 为了说明选通网络的实现方式，我们定义一下记号：\n",
    "\n",
    "inputs.shape = [batch_size, seq_len, embed_size] = [1, 8, 16] \n",
    "\n",
    "即输入有batch_size=1个数据点，该数据有seq_len长度的context，即包含seq_len=8个Embedding，每个Embedding长度为embed_dim=16。\n",
    "\n",
    "记 num_expert = 4, 即该层包含 num_expert 个并列的Expert。\n",
    "\n",
    "记 active_expert = 2, 即计算每个Embedding仅有 active_expert 个Expert 参与计算。\n",
    "\n",
    "### 选通网络计算\n",
    "对于有seq_len=8的数据，如果每个Expert都参与计算每一个Embedding，那么一共需要计算 seq_len*embed_size = 32 次， 这极大的增加了模型计算量，因此我们往往只激活其中的active_experts个Expert，这要求我们对每一个Embedding计算最合适的active_experts个 Expert。\n",
    "\n",
    "对于单个Expert 的原版Transformer来说：\n",
    "\n",
    "$$\n",
    "outputs[0,seq] = FeedForward(inputs[0,seq])\n",
    "$$\n",
    "\n",
    "对于多个Expert的网络：\n",
    "\n",
    "$$\n",
    "outputs[0,seq] = \\sum_{i \\in range(num\\_model)} \\alpha_{i} Expert_{i}(inputs[0,seq])\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha_{i} = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "    1 & Expert_{i}  \\text{is selected} \\\\\n",
    "    0 & Expert_{i}  \\text{is not selected} \\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "将$\\{\\alpha_0,\\alpha_1,\\dots,\\alpha_{num_experts-1}\\}$记为向量$\\alpha$:\n",
    "$$\n",
    "outputs[0,seq] = \\alpha \\cdot \\{Expert_i(inputs[0,seq])\\}\n",
    "$$\n",
    "\n",
    "一个选通0,2号Expert的$\\alpha$的例子是$[1,0,1,0]$\n",
    "\n",
    "问题在于如何求得 $\\alpha$, 对于一个Embedding ，我们使用神经网络对每个Expert打分，在根据分数计算$\\alpha$\n",
    "\n",
    "$$\n",
    "score[0,seq] = MLP(inputs[0,seq])  \\\\\n",
    "\\alpha = topK(score[0,seq])\n",
    "$$\n",
    "\n",
    "例如：\n",
    "\n",
    "$$\n",
    "score[0,seq] = [11.32,1.54,14.83,-1.90] \\\\\n",
    "\\alpha = [1,0,1,0]\n",
    "$$\n",
    "\n",
    "从优化的角度来说，$\\alpha$取前k大的分数的下标（即argmax），这个操作是不可导的，这里我们用之前在\"attention.ipynb\"中提到的技巧处理这里的计算。\n",
    "\n",
    "$$\n",
    "mask(score[0,seq]) = [11.32,-inf,14.83,-inf] \\\\\n",
    "\\alpha = softmax(mask(score[0,seq])) = [0.028,0,0.971,0] \\\\\n",
    "index = [1,0,1,0]\n",
    "$$\n",
    "\n",
    "我们用这个$\\alpha$和$index$用做选通网络."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First define the top k router module\n",
    "class TopkRouter(nn.Module):\n",
    "    def __init__(self, embed_size, num_experts, active_experts):\n",
    "        ## TODO\n",
    "        ## embed_size : dimension of embedding \n",
    "        ## num_experts : how many Experts per layer\n",
    "        ## active_experts: only active_experts out of num_experts are selected to process Embeddings per token.\n",
    "        super(TopkRouter, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_experts = num_experts\n",
    "        self.active_experts = active_experts\n",
    "        self.fc = nn.Linear(embed_size, num_experts)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ## TODO\n",
    "        ## 完成这部分时，注意使用Softmax()对router_output做标准化。同时注意这部分所用操作的可导性。\n",
    "        ## 输入值\n",
    "        ## inputs is the output tensor from multihead self attention block, shape (B:batch size, T: seq_len, C: embed_size)\n",
    "        ## 返回值\n",
    "        ## router_output: normalized weight of Experts, 即教程中的 \\alpha\n",
    "        ## indices:   index of selected Experts, 即教程中的 index\n",
    "        router_output = self.softmax(self.fc(inputs))\n",
    "        top_values, indices = torch.topk(router_output, self.active_experts, dim=-1)\n",
    "        return router_output, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 稀疏专家网络 SparseMoE\n",
    "\n",
    "![moe](./moeSparse.png)\n",
    "\n",
    "在定义完Expert 和 TopkRouter后，我们可以定义SparseMoE模块。\n",
    "\n",
    "在前向过程中，对于inputs.shape = [Batch_size,seq_len,embed_size]第二维度seq_len个Embedding,我们先利用TopkRouter计算出选通专家序号indices以及专家权重router_output。\n",
    "\n",
    "我们将Embedding通过选通的Expert得出active_expert个新的Embedding，然后使用router_output的作为权重对新的Embedding加权求和作为输出。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, embed_size:int, num_experts:int, active_experts:int):\n",
    "        ## TODO\n",
    "        super(SparseMoE, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.active_experts = active_experts\n",
    "        self.experts = nn.ModuleList([Expert(embed_size) for _ in range(num_experts)])\n",
    "        self.router = TopkRouter(embed_size, num_experts, active_experts)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ## TODO\n",
    "        router_output, indices = self.router(inputs)\n",
    "        expert_outputs = [expert(inputs) for expert in self.experts]\n",
    "        expert_outputs = torch.stack(expert_outputs, dim=1)\n",
    "        final_output = torch.einsum('bnte,btn->bte', expert_outputs, router_output)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer由一层层的block堆叠而成，其中每个block的结构从模型的结构图展开中可以看到，由LayerNorm，Masked multi head attention，(SparseMoE)FeedForward组成。\n",
    "\n",
    "对于一个表示句子的输入向量x，其首先会经过Layer Normalization层.\n",
    "Layer Normalization 层对于一个 句子个数x句子长度x单词向量维度 的输入 x, 会在最后两维上进行规范化处理，起到稳定训练的作用。\n",
    "\n",
    "$$\n",
    "LN(x)=\\frac{x-mean(x)}{\\sqrt{var(x)+\\epsilon}}\\cdot\\gamma+\\beta\n",
    "$$\n",
    "\n",
    "其中mean和var都是在最后两个维度上进行的，layernorm的实现同学们可以直接调用nn.LayerNorm\n",
    "经过layernorm层后，再经过Mask multi head attention层之后，会在+号处再次和原始的输入进行相加，这样的做法能够提高训练的稳定性。有兴趣的同学可以从梯度角度思考原因，或者搜索残差连接相关资料进行学习。\n",
    "之后再同样经过一层layernorm和feedforwad之后，就可以得到block块的输出了。\n",
    "即 x' = x+MHA(LN(x)), y = FFN(LN(x'))+x'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    # Transformer basic block, consist of MultiHeadAttention, FeedForward and layer normalization\n",
    "    def __init__(self, embed_size:int, n_heads:int, seq_len:int, num_experts:int, active_experts:int):\n",
    "        super().__init__()\n",
    "        # TODO: implement block structure\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.mha = MultiHeadAttention(n_heads, embed_size//n_heads, seq_len, embed_size)\n",
    "        self.ff = SparseMoE(embed_size, num_experts, active_experts)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # input: (batch_size, seq_len, embed_size)\n",
    "        #TODO: forward with residual connection\n",
    "        x = self.mha(inputs)\n",
    "        x = self.norm1(x + inputs)\n",
    "        x = self.ff(x)\n",
    "        return self.norm2(x + inputs)\n",
    "\n",
    "class SparseMoETransformer(nn.Module):\n",
    "    # Transformer decoder, consist of \n",
    "    # token embedding layer and position_embedding(position_embedding 可以理解为对位置编码，感兴趣的同学可以查阅原文，这里可以看为vocab_len = seq_len的Embedding)\n",
    "    # a stack of Transformer basic block\n",
    "    # a layernorm and output linear layer\n",
    "    def __init__(self, vocab_size:int, seq_len:int, embed_size:int, n_layers:int, n_heads:int, num_experts:int, active_experts:int):\n",
    "        # vocab_size is the number of word in vocabulary dict\n",
    "        # seq_len is the sequence length/sentence length\n",
    "        # embed_size is the embedding vector dimension\n",
    "        super().__init__()\n",
    "        # TODO: \n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(seq_len, embed_size)\n",
    "        self.blocks = nn.ModuleList([Block(embed_size, n_heads, seq_len, num_experts, active_experts) for _ in range(n_layers)])\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(vocab_size)\n",
    "        self.fc = nn.Linear(embed_size, vocab_size)\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, inputs, labels=None):\n",
    "        # labels: the (ground) true output \n",
    "        # TODO: implement the forward function of the transformer\n",
    "\n",
    "        # inputs:(batch_size, seq_len, )\n",
    "        batch_size, seq_len, = inputs.shape\n",
    "        # embedding:(batch_size, seq_len, embed_size)\n",
    "        embedding = self.token_embedding(inputs) + self.position_embedding(torch.arange(seq_len, device=device))\n",
    "\n",
    "        # attens:(batch_size, seq_len, embed_size)\n",
    "        attens = embedding\n",
    "        for block in self.blocks:\n",
    "            attens = block(attens)\n",
    "\n",
    "        # logits:(batch_size, seq_len, vocab_size)\n",
    "        logits = self.fc(attens)\n",
    "        logits = self.norm2(logits)\n",
    "        logits = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        # compute the loss\n",
    "        \n",
    "        if labels is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits = logits.view(batch_size * seq_len, vocab_size)\n",
    "            labels = labels.view(batch_size * seq_len)\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "        return logits, loss\n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        inputs = torch.tensor(tokenizer.encode(inputs)).unsqueeze(0)\n",
    "        device = next(self.parameters()).device  \n",
    "        inputs = inputs.to(device)\n",
    "        if inputs.size(1) > self.seq_len:\n",
    "            inputs = inputs[:, :self.seq_len]\n",
    "        elif inputs.size(1) < self.seq_len:\n",
    "            inputs = F.pad(inputs, (self.seq_len - inputs.size(1), 0, 0, 0))\n",
    "        generated = inputs\n",
    "        for _ in range(max_new_tokens):\n",
    "            if generated.size(1) > self.seq_len:\n",
    "                generated_input = generated[:, -self.seq_len:]\n",
    "            else:\n",
    "                generated_input = generated\n",
    "            logits, _ = self.forward(generated_input)\n",
    "            last_logits = logits[:, -1, :]  \n",
    "            next_token_ids = torch.argmax(last_logits, dim=-1)  \n",
    "            next_token_ids = next_token_ids.unsqueeze(-1)  \n",
    "            generated = torch.cat([generated, next_token_ids], dim=1)  \n",
    "        return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练循环\n",
    "\n",
    "如果你已经完成了模型定义等内容，训练的过程实际上在高度封装的 Pytorch 库中非常简单, 因为你并不需要写对应的反向传播。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss \n",
    "\n",
    "Loss 用来**衡量**模型预测与真实值之间的**差距**。\n",
    "\n",
    "常见的几个 Loss 函数：\n",
    "\n",
    "* 交叉熵：$\\text{CrossEntropy Loss} = -\\sum_{i=1}^{n} y_i \\log(\\hat{y}_i)$\n",
    "* 均方误差：$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$\n",
    "* 绝对误差：$\\text{MAE} = \\frac{1}{n}\\sum_{i=1}^{n} |y_i - \\hat{y_i}|$\n",
    "\n",
    "不同的 loss 对应不同的优化目标，如果写错 loss 函数会导致模型不收敛/性能很差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练循环\n",
    "\n",
    "当我们写好 Optimizer 和 Loss 之后，对应的训练循环就十分简单了。\n",
    "\n",
    "我们只需要做以下事情：\n",
    "\n",
    "* 从 dataloader 里面拿到一个 batch 的数据以及标签\n",
    "* 将数据送入模型，进行前向传播\n",
    "* 拿到模型输出的 logits\n",
    "* 将 logits 和 标签进行 loss 计算\n",
    "* 用 Optimizer \n",
    "    * 清空梯度\n",
    "    * 反向传播\n",
    "    * 更新参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, epoch, device):\n",
    "    # Optimizer 会根据模型的输出和真实标签计算梯度，然后利用反向传播算法更新模型的参数。\n",
    "    # 在本实验中你可以将 Optimizer 视作黑盒，只需要知道如何使用即可。\n",
    "    # 找一个合适的 Optimizer。对不同的任务，模型，最适合的优化器是不一样的，你可以先尝试最常用的 Adam，如果有兴趣可以看看其他的优化器。\n",
    "    # docs see: https://pytorch.org/docs/stable/optim.html \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    from tqdm import tqdm\n",
    "    for i, (inputs, targets) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # TODO: implement the training process, and compute the training loss and validation loss\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        logits, loss = model(inputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "\n",
    "\n",
    "    print(f'Epoch {epoch} Loss: {total_loss / len(dataloader)}')\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate(model, dataloader, epoch, device):\n",
    "    model.eval()\n",
    "    # TODO: 实现验证函数。与训练函数类似，但不需要计算梯度。\n",
    "    total_loss = 0\n",
    "    from tqdm import tqdm\n",
    "    for i, (inputs, targets) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        logits, loss = model(inputs, targets)\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1743/1743 [01:39<00:00, 17.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 1.7344252352596903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 436/436 [00:07<00:00, 57.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train Loss: 1.7344252352596903, Valid Loss: 1.4295976536536434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1743/1743 [01:38<00:00, 17.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.3806652530335313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 436/436 [00:07<00:00, 57.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train Loss: 1.3806652530335313, Valid Loss: 1.3393083283113778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1743/1743 [01:38<00:00, 17.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.3135619261315412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 436/436 [00:07<00:00, 56.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train Loss: 1.3135619261315412, Valid Loss: 1.2877376205330595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1743/1743 [01:38<00:00, 17.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.26825657112463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 436/436 [00:07<00:00, 56.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Train Loss: 1.26825657112463, Valid Loss: 1.2501382179763338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1743/1743 [01:38<00:00, 17.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 1.2318726025021631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 436/436 [00:07<00:00, 56.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Train Loss: 1.2318726025021631, Valid Loss: 1.219027877947606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1743/1743 [01:38<00:00, 17.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 1.2016494923190557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 436/436 [00:07<00:00, 55.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Train Loss: 1.2016494923190557, Valid Loss: 1.1902902968432925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1743/1743 [01:38<00:00, 17.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 1.1740452509396937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 436/436 [00:07<00:00, 56.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Train Loss: 1.1740452509396937, Valid Loss: 1.1707300571126675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1743/1743 [01:38<00:00, 17.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 1.1504231819939628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 436/436 [00:07<00:00, 56.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Train Loss: 1.1504231819939628, Valid Loss: 1.1442745475594056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1743/1743 [01:38<00:00, 17.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: 1.1291048887195905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 436/436 [00:07<00:00, 55.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Train Loss: 1.1291048887195905, Valid Loss: 1.1240164211036963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1743/1743 [01:38<00:00, 17.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss: 1.1098666473564853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 436/436 [00:07<00:00, 56.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Train Loss: 1.1098666473564853, Valid Loss: 1.1055239551111098\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1049/829430014.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(tokenizer.encode(inputs)).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I could pick my lances\n",
      "As the season of the season of the seas.\n",
      "\n",
      "BRUTUS:\n",
      "We stand upon the strange of the world,\n",
      "I will b\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader('input.txt', tokenizer, chunk_size=50, batch_size=512)\n",
    "model = SparseMoETransformer(vocab_size=len(tokenizer.char2index)+1, seq_len=50, embed_size=64, n_layers=3, n_heads=8, num_experts=8, active_experts=2).to(device)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_loss(train_loss, valid_loss):\n",
    "    plt.plot(train_loss, label='train')\n",
    "    plt.plot(valid_loss, label='valid')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# 训练模型\n",
    "def run(model, train_dataloader, valid_dataloader, device, epochs=10):\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train(model, train_dataloader, epoch, device)\n",
    "        valid_loss = validate(model, valid_dataloader, epoch, device)\n",
    "        train_loss_list.append(train_loss)\n",
    "        valid_loss_list.append(valid_loss)\n",
    "        print(f'Epoch {epoch} Train Loss: {train_loss}, Valid Loss: {valid_loss}')\n",
    "    plot_loss(train_loss_list, valid_loss_list)\n",
    "\n",
    "#TODO: 用 matplotlib plot 训练过程中的 loss 变化\n",
    "\n",
    "train_dataloader, val_dataloader = dataloader\n",
    "\n",
    "run(model, train_dataloader, val_dataloader, device, epochs=10)\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "print(tokenizer.decode(model.generate(\"I could pick my lance\",max_new_tokens=100)[0].tolist()).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1049/829430014.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(tokenizer.encode(inputs)).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is nothing either good or bad,\n",
      "And the thing that thou hast should be so sound.\n",
      "\n",
      "LADY GREY:\n",
      "The time to me that made thee to make thee with his\n",
      "conger, and the man that we have been a bark.\n",
      "\n",
      "MENENIUS:\n",
      "I will not stay the words th\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "print(tokenizer.decode(model.generate(\"There is nothing either good or bad\",max_new_tokens=200)[0].tolist()).strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
